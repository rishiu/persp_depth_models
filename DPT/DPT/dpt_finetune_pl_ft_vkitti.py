# -*- coding: utf-8 -*-
"""DPT_snow_snow_05_24.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1APvcXwV9brmAJDadLZHm3k-6r5loySDo

# Note

LEARNING RATE
  set to what it is in the paper, but the paper doesn't say for fine-tuning
  no learning rate scheduler

OPTIMIZER
  The paper says they use Multi-Objective Optimization with Adam optimizer
  Not sure what multiple objectives are used 
  (possibly has something to do with dataset mixing, not in fine-tuning)
  Used Adam optimizer only

BATCH SIZE
  They used batch size 16
  We ran out of memory, so used batch size 8

SCALE AND SHIFT WITH INVERSION
  We changed one line of code
  We invert the image first, then do the calculated scale and shift
  This is because our lidar images have really small values (1e-7)
  So we cannot invert those to calculate pre-invert scale and shift

# Mount Drive
"""

# Commented out IPython magic to ensure Python compatibility.
# Mount the google drive
#from google.colab import drive
#drive.mount('/content/gdrive', force_remount=True)

# cd only once to the wanted dir at the beginning
# %cd '/content/gdrive/Shareddrives/SnowGeneration'

"""# Copy Data

"""

# Creates the directories needed to unzip dataset
#!mkdir /content/Dataset

# Unzips the training and validation dataset
# Change if you want to use a different Dataset

#!cp ./Data/dense_Depth_MonoDepth.zip /content/Dataset/ 
#!unzip -q /content/Dataset/dense_Depth_MonoDepth.zip -d /content/Dataset/
#!rm /content/Dataset/dense_Depth_MonoDepth.zip

"""# Install Package"""

#!pip install -r DPT/requirements.txt
#!pip install tensorboard
#!pip install tensorboardX
#!pip install tensorflow

"""# Library"""

# Commented out IPython magic to ensure Python compatibility.
"""Compute depth maps for images in the input folder.
"""
import sys
sys.path.append('/content/gdrive/Shareddrives/SnowGeneration/DPT')
import os
import cv2
import argparse
from PIL import Image, ImageChops, ImageOps, ImageEnhance
import random
from natsort import natsorted
from glob import glob
from pathlib import Path

#from util import io

from dpt.models import DPTDepthModel
from dpt.transforms import Resize, NormalizeImage, PrepareForNet
from validate_kitti import KittiDepthSelectionV2

sys.path.append("../../vidar/")
from vidar.metrics.depth import DepthEvaluationV2

import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms.functional as TF
from torch.autograd import Variable
import functools
from torch.nn import init
import torchvision
from torchvision.transforms import Compose

from tqdm import tqdm
import numpy as np
from tensorboardX import SummaryWriter
# %load_ext tensorboard
# %matplotlib inline
import matplotlib.pyplot as plt
import pytorch_lightning as pl
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint

"""# Helper"""

# Plot image
def show_img(img, title=None, figsize=(15, 15)):
  #img = (img+1)/2
  plt.figure(figsize=figsize)
  plt.imshow(img)
  if title:
    plt.title(title)
  plt.show()

"""# Dataloader"""

# DataLoaders for Training and Validation set
class DPTData(Dataset):
  """
    The dataset class for weather net training and validation.

    Parameters:
        root_dir_list (list) -- list of dirs for the dataset.
        is_train (bool) -- True for training set.
  """
  def __init__(self, root_dir_list, is_train=True):
    super(DPTData, self).__init__()

    self.is_train = is_train
    self.img_paths = []
    self.depth_paths = []
    for root_dir in root_dir_list:
      self.img_paths += natsorted(glob(f"{root_dir}/img/*.jpg"))
      self.depth_paths += natsorted(glob(f"{root_dir}/depth/*.npy"))
       
    # number of images
    self.data_len = len(self.img_paths)
    print(self.data_len)
  
  def __len__(self):
    # return 16
    return self.data_len

  def get_scene_indices(self):
    return self.scene_indices
  
  def __getitem__(self, index):
    inp_path = self.img_paths[index]
    inp_img = Image.open(inp_path)
    filename = inp_path.split('/')[-1][:-4]

    depth_path = self.depth_paths[index]
    #depth_img = Image.open(depth_path)
    depth_img = np.load(depth_path)

    # To numpy
    inp_img = np.array(inp_img, dtype=np.float32)
    depth_img = np.array(depth_img, dtype=np.float32)

    if not np.isfinite(inp_img).all() or not np.isfinite(depth_img).all():
      print("Non finite!")

    #print(depth_img.shape)
    #print(inp_img.shape)

    inp_img *= 1/255.0
    #depth_img /= np.max(depth_img)
    # Crop Code
    h = inp_img.shape[0]
    w = inp_img.shape[1]

    dh = depth_img.shape[0]
    dw = depth_img.shape[1]
    # Crop out top 200
    #inp_img = inp_img[200:h, :, :]
    #lidar_img = lidar_img[200:h, :]
    # Resize to height 384
    #inp_img = cv2.resize(inp_img, (384, 384), interpolation=cv2.INTER_NEAREST)
    #lidar_img = cv2.resize(lidar_img, (1058, 384), interpolation=cv2.INTER_NEAREST)
    # Random Crop for square
    if h > 384 and w > 384 and dh > 384 and dw > 384:
      cc_x = random.randint(0, 512-384)
      cc_y = random.randint(0, 512-384)
      inp_img = inp_img[cc_y:cc_y+384, cc_x:cc_x+384,:]
      depth_img = np.expand_dims(depth_img, axis=2)
      depth_img = depth_img[cc_y:cc_y+384, cc_x:cc_x+384,:]
    else:
      inp_img = cv2.resize(inp_img, (384,384))
      depth_img = cv2.resize(depth_img, (384,384))
      depth_img = np.expand_dims(depth_img, axis=2)


    inp_img = (inp_img - .5) / .5

    inp_img = torch.from_numpy(inp_img).permute((2,0,1))
    depth_img = torch.from_numpy(depth_img).permute((2,0,1))

    # Data augmentations: flip x, flip y, rotate by (90, 180, 270), combinations of flipping and rotating
    if self.is_train:
      aug = random.randint(0, 1)
    else:
      aug = 0
    
    if aug==1:
      inp_img = inp_img.flip(2)
      depth_img = depth_img.flip(2)

    # Dict for return
    # If using tanh as the last layer, the range should be [-1, 1]
    sample_dict = {
        'input_img': inp_img,
        'depth_img': depth_img,
        'file_name': filename
    }

    return sample_dict

# # Create the DataLoaders for training and validation
# train_dataset = DPTData(
#     root_dir_list=params['root_dir_list'],
#     is_train=True
# )

# train_loader = DataLoader(
#     dataset=train_dataset,
#     batch_size=1, 
# )

# for i in train_loader:
#   break

"""# Secheduler"""

"""
Linear warmup from: 
https://github.com/ildoonet/pytorch-gradual-warmup-lr/blob/master/warmup_scheduler/scheduler.py
"""

from torch.optim.lr_scheduler import _LRScheduler
from torch.optim.lr_scheduler import ReduceLROnPlateau

class GradualWarmupScheduler(_LRScheduler):
  """ Gradually warm-up(increasing) learning rate in optimizer.
  Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.
  Args:
      optimizer (Optimizer): Wrapped optimizer.
      multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.
      total_epoch: target learning rate is reached at total_epoch, gradually
      after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
  """

  def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):
    self.multiplier = multiplier
    if self.multiplier < 1.:
      raise ValueError('multiplier should be greater thant or equal to 1.')
    self.total_epoch = total_epoch
    self.after_scheduler = after_scheduler
    self.finished = False
    super(GradualWarmupScheduler, self).__init__(optimizer)

  def get_lr(self):
    if self.last_epoch > self.total_epoch:
      if self.after_scheduler:
        if not self.finished:
          self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]
          self.finished = True
        return self.after_scheduler.get_last_lr()
      return [base_lr * self.multiplier for base_lr in self.base_lrs]

    if self.multiplier == 1.0:
      return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]
    else:
      return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]

  def step_ReduceLROnPlateau(self, metrics, epoch=None):
    if epoch is None:
      epoch = self.last_epoch + 1
    self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning
    if self.last_epoch <= self.total_epoch:
      warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]
      for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):
        param_group['lr'] = lr
    else:
      if epoch is None:
        self.after_scheduler.step(metrics, None)
      else:
        self.after_scheduler.step(metrics, epoch - self.total_epoch)

  def step(self, epoch=None, metrics=None):
    if type(self.after_scheduler) != ReduceLROnPlateau:
      if self.finished and self.after_scheduler:
        if epoch is None:
          self.after_scheduler.step(None)
        else:
          self.after_scheduler.step(epoch - self.total_epoch)
        self._last_lr = self.after_scheduler.get_last_lr()
      else:
        return super(GradualWarmupScheduler, self).step(epoch)
    else:
      self.step_ReduceLROnPlateau(metrics, epoch)

"""# Loss"""

"""
LOSS FUNCTIONS
"""
class ScaleInvariantLoss(nn.Module):
  #Scale Invariant Loss according to https://arxiv.org/pdf/1406.2283.pdf

  def __init__(self, eps=.5):
    super(ScaleInvariantLoss, self).__init__()
    self.eps = eps
  
  def forward(self, est, tar):
    diff = (torch.log(tar) - torch.log(est))
    l2 = torch.mean(torch.square(diff))
    scale_inv = torch.sum(diff)**2
    num_pixels = tar.size(dim=0)
    total = l2 - (self.eps/(num_pixels**2))*scale_inv
    return total

class SSITRIM(nn.Module):
  def __init__(self):
    super(SSITRIM, self).__init__()

  def forward(self, d, d_star):
    M = d.shape[2]*d.shape[3]
    M_80 = int(0.8 * M)
    d = d.flatten(start_dim=1)
    d_star = d_star.flatten(start_dim=1)
    t_d = torch.median(d, dim=1)[0]
    s_d = (torch.sum(torch.abs(d - t_d.unsqueeze(1))) + 1e-8) / M
    d = (d - t_d.unsqueeze(1)) / s_d
    t_dstar = torch.median(d_star, dim=1)[0]
    s_dstar = (torch.sum(torch.abs(d_star - t_dstar.unsqueeze(1))) + 1e-8) / M
    d_star = (d_star - t_dstar.unsqueeze(1)) / s_dstar
    diff = torch.abs(d - d_star)
    filter_diff = torch.topk(diff, k=M_80, dim=1, largest=False)[0]
    loss = torch.sum(filter_diff) / (2 * M)
    return loss

def reduction_batch_based(image_loss, M):
    # average of all valid pixels of the batch

    # avoid division by 0 (if sum(M) = sum(sum(mask)) = 0: sum(image_loss) = 0)
    divisor = torch.sum(M)

    if divisor == 0:
        return 0
    else:
        return torch.sum(image_loss) / divisor

def gradient_loss(prediction, target, mask, reduction=reduction_batch_based):

    M = torch.sum(mask, (1, 2))

    diff = prediction - target
    diff = torch.mul(mask, diff)

    grad_x = torch.abs(diff[:, :, 1:] - diff[:, :, :-1])
    mask_x = torch.mul(mask[:, :, 1:], mask[:, :, :-1])
    grad_x = torch.mul(mask_x, grad_x)

    grad_y = torch.abs(diff[:, 1:, :] - diff[:, :-1, :])
    mask_y = torch.mul(mask[:, 1:, :], mask[:, :-1, :])
    grad_y = torch.mul(mask_y, grad_y)

    image_loss = torch.sum(grad_x, (1, 2)) + torch.sum(grad_y, (1, 2))

    return reduction(image_loss, M)

class GradientLoss(nn.Module):
    def __init__(self, scales=4, reduction='batch-based'):
        super().__init__()

        if reduction == 'batch-based':
            self.__reduction = reduction_batch_based

        self.__scales = scales

    def forward(self, prediction, target, mask):
        total = 0

        for scale in range(self.__scales):
            step = pow(2, scale)

            total += gradient_loss(prediction[:, ::step, ::step], target[:, ::step, ::step],
                                   mask[:, ::step, ::step], reduction=self.__reduction)

        return total

def mse_loss(prediction, target, mask, reduction=reduction_batch_based):

    M = torch.sum(mask, (1, 2))
    res = prediction - target
    image_loss = torch.sum(mask * res * res, (1, 2))

    return reduction(image_loss, 2 * M)

class MSELoss(nn.Module):
    def __init__(self, reduction='batch-based'):
        super().__init__()

        if reduction == 'batch-based':
            self.__reduction = reduction_batch_based

    def forward(self, prediction, target, mask):
        return mse_loss(prediction, target, mask, reduction=self.__reduction)

class ScaleAndShiftInvariantLoss(nn.Module):
    def __init__(self, alpha=0.5, scales=4, reduction='batch-based'):
        super().__init__()

        self.__data_loss = MSELoss(reduction=reduction)
        #self.__regularization_loss = GradientLoss(scales=scales, reduction=reduction)
        self.__alpha = alpha

        self.__prediction_ssi = None

    def compute_scale_and_shift(self, prediction, target, mask):
      # system matrix: A = [[a_00, a_01], [a_10, a_11]]
      a_00 = torch.sum(mask * prediction * prediction, (1, 2))
      a_01 = torch.sum(mask * prediction, (1, 2))
      a_11 = torch.sum(mask, (1, 2))

      # right hand side: b = [b_0, b_1]
      b_0 = torch.sum(mask * prediction * target, (1, 2))
      b_1 = torch.sum(mask * target, (1, 2))

      # solution: x = A^-1 . b = [[a_11, -a_01], [-a_10, a_00]] / (a_00 * a_11 - a_01 * a_10) . b
      x_0 = torch.zeros_like(b_0)
      x_1 = torch.zeros_like(b_1)

      det = a_00 * a_11 - a_01 * a_01
      valid = det.nonzero()

      x_0[valid] = (a_11[valid] * b_0[valid] - a_01[valid] * b_1[valid]) / det[valid]
      x_1[valid] = (-a_01[valid] * b_0[valid] + a_00[valid] * b_1[valid]) / det[valid]

      return x_0, x_1

    def forward(self, prediction, target):
        #preprocessing
        mask = target > 0

        #calcul
        scale, shift = self.compute_scale_and_shift(prediction, target, mask)
        # print(scale, shift)
        self.__prediction_ssi = scale.view(-1, 1, 1) * prediction + shift.view(-1, 1, 1)

        total = self.__data_loss(self.__prediction_ssi, target, mask)
        #if self.__alpha > 0:
        #    total += self.__alpha * self.__regularization_loss(self.__prediction_ssi, target, mask)

        return total

    def __get_prediction_ssi(self):
        return self.__prediction_ssi

    prediction_ssi = property(__get_prediction_ssi)

class ScaleInvariantGradientLoss(nn.Module):
  def __init__(self, scale):
    super(ScaleInvariantGradientLoss, self).__init__()
    self.scales = []
    for i in range(scale):
      self.scales.append(1 / (2**i))

  def interpolate(self, img, scale):
    return torch.nn.functional.interpolate(img, scale_factor=scale, mode="bilinear")

  def standardize(self, arr):
    M = arr.shape[2] * arr.shape[3]
    t = torch.median(arr.flatten(start_dim=1), dim=1)[0]
    t = t.reshape(-1,1,1,1)
    #print(torch.max(arr), torch.min(arr))
    s = torch.sum(torch.abs(arr - t)) / M
    #print(s)
    arr = (arr - t) / s
    return arr

  def forward(self, d, d_star):
    loss = torch.zeros(d.shape[0], device="cuda")
    #print(torch.max(d), torch.min(d))
    d = self.standardize(d)
    d_star = self.standardize(d_star)

    for s in self.scales:
      d_resize = self.interpolate(d, s)
      d_star_resized = self.interpolate(d_star, s)

      R = d_resize - d_star_resized

      R_x = torch.mean(torch.abs(R[:,:,:,:-1] - R[:,:,:,1:]), axis=(1,2,3))
      R_y = torch.mean(torch.abs(R[:,:,:-1,:] - R[:,:,1:,:]), axis=(1,2,3))

      R_grad = R_x + R_y

      loss += R_grad
    mloss = torch.mean(loss)
    return mloss



def SmoothLoss(disp, img):
  """Computes the smoothness loss for a disparity image
  The color image is used for edge-aware smoothness
  """
  grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])
  grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])

  grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)
  grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)

  grad_disp_x *= torch.exp(-grad_img_x)
  grad_disp_y *= torch.exp(-grad_img_y)

  return grad_disp_x.mean() + grad_disp_y.mean()

"""# Training Parameters"""

# Training config parameters
# may want to move to a .yml file later, done like this for now for simplicity

# Parameters
params = {
  'batch_size': 16, # batch size
  'num_epochs': 100, # number of epochs to train
  'warmup_epochs': 4, # number of epochs for warmup
  'initial_lr': 1e-6, # initial learning rate used by scheduler
  'backbone_lr': 1e-7, # lr for
  'min_lr': 1e-8, # minimum learning rate used by scheduler
  'val_epoch': 1, # validation done every k epochs
  'save_every': 5, # save every k epoch
  'save_dir': './checkpoints/fixed_loss_ckpts/full_data_ft_vkitti/',
  'root_dir_list': [
      '/usr/project/gen_data/generated_data/vKITTI_FT/'
    ], # Dir for the training data
  'val_dir_list': [
      '/usr/project/gen_data/generated_data/LATEST_FINAL_OUT/FT_val/'
    ], # Dir for the validation data
  'scale':  39.408864690046954, # scale for Dense Dataset
  'trans' : 0.423200181435787, # translation for Dense Dataset
  'masked_ssi_loss_weight': 1, # weight for masked l1 loss (only where lidar points are)
  'smooth_loss_weight': False, # weight for smooth loss
  'ssi_trim_loss_weight': False,
  'scale_inv_grad_loss_weight': False,
  'resume_train': False, # begin training using loaded checkpoint
  'model_path': './checkpoints/dpt_hybrid_midas/dpt_hybrid-midas-501f0c75.pt', # Dir to load model weights
  'tensorboard_log_step_train': 100, # Number of steps to log into tensorboard when training
  'tensorboard_log_step_val': 1, # This number will be updated automatically based after creating the dataloaders
}

class DPTFinetune(pl.LightningModule):
  def __init__(self):
    super().__init__()
    self.model = DPTDepthModel(
            path=params['model_path'],
            scale=1,#params['scale'],
            shift=0,#params['trans'],
            invert=False,
            backbone="vitb_rn50_384",
            non_negative=True,
            enable_attention_hooks=False,
        )

    self.criterion_masked_ssi = ScaleAndShiftInvariantLoss()
    self.scale_inv_grad_loss = ScaleInvariantGradientLoss(scale=4)

  def forward(self, x):
    out = self.model(x)
    #out = out * params['scale'] + params['trans']
    out = torch.unsqueeze(out, 1)
    return out

  def configure_optimizers(self):
    backbone_params = []
    decoder_params = []
    for cur_name, parameters in self.model.named_parameters():
      if 'patch_embed' in cur_name or 'blocks' in cur_name:
        backbone_params.append(parameters)
      else:
        decoder_params.append(parameters)

    optimizer = optim.Adam(
      [{"params": decoder_params},
      {"params": backbone_params, "lr": params['initial_lr'] / 10}], 
      lr=params['initial_lr'], 
      betas=(0.9, 0.999), 
      eps=1e-8
    )

    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(
      optimizer, 
      params['num_epochs'], 
      eta_min=params['min_lr'])

    #scheduler = GradualWarmupScheduler(
    #  optimizer, 
    #  multiplier=1.0, 
    #  total_epoch=params['warmup_epochs'], 
    #  after_scheduler=scheduler_cosine)

    return [optimizer], [scheduler_cosine]

  def get_sum_loss(self, net_out, gt, criterion):
    if isinstance(net_out, list):
      loss = 0
      for net_out_i in net_out:
        loss += criterion(net_out_i, gt)
    else:
      loss = criterion(net_out, gt)
    return loss

  def training_step(self, train_batch, train_idx):
    input_img = train_batch['input_img'].cuda()
    depth_img = train_batch['depth_img'].cuda()

    out = self.forward(input_img)

    depth_img = torch.clip(depth_img, 1e-8)
    out = out + 1e-8

    loss = 0.0

    mask = (depth_img > 0) & (out > 0)

    mask_depth = (depth_img > 0)
    mask_out = (out > 0)

    if params['masked_ssi_loss_weight']:
      #out_masked = out[mask]
      #depth_img_masked = depth_img[mask]
      out_masked = out.squeeze(1)
      depth_img_masked = depth_img.squeeze(1)
      loss_ssi_masked = self.get_sum_loss(out_masked, depth_img_masked, self.criterion_masked_ssi)
      loss += params['masked_ssi_loss_weight'] * loss_ssi_masked
      loss_ssi_masked_log = loss_ssi_masked.item()
      self.log('train/ssi_masked', loss_ssi_masked_log)

    if params['smooth_loss_weight']:
      loss_smooth = SmoothLoss(out, input_img)
      loss += params['smooth_loss_weight'] * loss_smooth
      loss_smooth_log = loss_smooth.item()
      self.log('train/smooth', loss_smooth_log)

    if params['ssi_trim_loss_weight']:
      loss_ssi_trim = self.get_sum_loss(out, depth_img, ssi_trim)
      loss += params['ssi_trim_loss_weight'] * loss_ssi_trim
      loss_ssi_trim_log = loss_ssi_trim.item()
      self.log('train/ssi_trim', loss_ssi_trim_log)

    if params['scale_inv_grad_loss_weight']:
      out_masked = torch.mul(out, mask)
      depth_img_masked = torch.mul(out, mask)
      loss_scale_inv_grad = self.get_sum_loss(out_masked, depth_img_masked, self.scale_inv_grad_loss)
      loss += params['scale_inv_grad_loss_weight'] * loss_scale_inv_grad
      loss_scale_inv_grad_log = loss_scale_inv_grad.item()
      self.log('train/scale_inv_grad', loss_scale_inv_grad_log)

    if train_idx % 500 == 0:
      input_img_plot = input_img[0,:,:,:].detach().cpu()
      input_img_plot = (input_img_plot + 1.0) / 2.0
      input_img_plot = np.transpose(input_img_plot, (1,2,0))
      out_plot = out[0, 0, :, :].detach().cpu()
      depth_img_plot = depth_img[0, 0, :, :].cpu()
      mask_plot = mask_depth[0,0,:,:].cpu()
      mask_plot2 = mask_out[0,0,:,:].cpu()
      fig, ax = plt.subplots(5)
      iim = ax[0].imshow(input_img_plot)
      plt.colorbar(iim)
      oim = ax[1].imshow(out_plot, cmap = 'jet')
      plt.colorbar(oim)
      dim = ax[2].imshow(depth_img_plot, cmap = 'jet')
      plt.colorbar(dim)
      mim = ax[3].imshow(mask_plot)
      m2im = ax[4].imshow(mask_plot2)
      plt.colorbar(m2im)
      plt.colorbar(mim)

      import time
      ti = time.time()
      plt.savefig("debug_outputs/epoch_ftvkitti_"+str(train_idx)+".jpg")
      plt.close()

    return loss

  def run_kitti_test(self):
    w = 384
    h = 384
    
    transform = Compose(
        [
            Resize(
                w,
                h,
                resize_target=False,
                keep_aspect_ratio=True,
                ensure_multiple_of=32,
                resize_method="minimal",
                image_interpolation_method=cv2.INTER_LINEAR
            ),
            NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
            PrepareForNet(),
        ]
    )

    ds = KittiDepthSelectionV2("/usr/project/depth_models/depth_data/KITTI_filtered/eigen_test/", "/usr/project/depth_models/depth_data/KITTI/", transform)
    dl = DataLoader(
       ds, batch_size=1, num_workers=1, shuffle=False, pin_memory=True
    )

    device = torch.device("cuda")

    evaluator = DepthEvaluationV2()

    overall_metrics = torch.zeros((1,8))

    with torch.no_grad():
      for i, batch in enumerate(tqdm(dl)):
        for k, v in batch.items():
          if k != "file_name" and k != "fname":
            batch[k] = v.to(device)

        pred = self.forward(batch["image"])

        pred = F.interpolate(
          pred,
          size=batch["mask"].shape[1:],
          mode="bilinear",
          align_corners=False,
        )
        pred = pred.squeeze(1)

        final_mask = batch["mask"]

        metrics = evaluator.compute(batch["depth"].unsqueeze(1), pred, use_gt_scale=True, mask=final_mask, idx=i)
        overall_metrics += metrics

    overall_metrics /= len(dl)

    print(overall_metrics)

    self.log('val/abs_rel', overall_metrics[0,0])
    self.log('val/sqr_rel', overall_metrics[0,1])
    self.log('val/rmse', overall_metrics[0,2])
    self.log('val/rmse_log', overall_metrics[0,3])
    self.log('val/silog', overall_metrics[0,4])
    self.log('val/a1', overall_metrics[0,5])
    self.log('val/a2', overall_metrics[0,6])
    self.log('val/a3', overall_metrics[0,7])

  def validation_step(self, val_batch, batch_idx):
    input_img = val_batch['input_img'].cuda()
    depth_img = val_batch['depth_img'].cuda()

    if batch_idx == 0:
      self.run_kitti_test()

    out = self.forward(input_img)

    depth_img = torch.clip(depth_img, 1e-8)

    loss = 0.0

    mask = (depth_img > 0) & (out > 0)

    if params['masked_ssi_loss_weight']:
      # out_masked = out[mask]
      # depth_img_masked = depth_img[mask]
      out_masked = out.squeeze(1)
      depth_img_masked = depth_img.squeeze(1)
      loss_ssi_masked = self.get_sum_loss(out_masked, depth_img_masked, self.criterion_masked_ssi)
      loss += params['masked_ssi_loss_weight'] * loss_ssi_masked
      loss_ssi_masked_log = loss_ssi_masked.item()
      self.log('val/ssi_masked', loss_ssi_masked_log)

    if params['smooth_loss_weight']:
      loss_smooth = SmoothLoss(out, input_img)
      loss += params['smooth_loss_weight'] * loss_smooth
      loss_smooth_log = loss_smooth.item()
      self.log('val/smooth', loss_smooth_log)

    if params['ssi_trim_loss_weight']:
      loss_ssi_trim = self.get_sum_loss(out, depth_img, ssi_trim)
      loss += params['ssi_trim_loss_weight'] * loss_ssi_trim
      loss_ssi_trim_log = loss_ssi_trim.item()
      self.log('val/ssi_trim', loss_ssi_trim_log)

    if params['scale_inv_grad_loss_weight']:
      loss_scale_inv_grad = self.get_sum_loss(out, depth_img, self.scale_inv_grad_loss)
      loss += params['scale_inv_grad_loss_weight'] * loss_scale_inv_grad
      loss_scale_inv_grad_log = loss_scale_inv_grad.item()
      self.log('val/scale_inv_grad', loss_scale_inv_grad_log)

    if batch_idx == 0:
      out_plot = out[0, 0, :, :].detach().cpu()
      depth_img_plot = depth_img[0, 0, :, :].cpu()
      mask_plot = mask[0,0,:,:].cpu()
      fig, ax = plt.subplots(3)
      oim = ax[0].imshow(out_plot, cmap = 'jet')
      plt.colorbar(oim)
      dim = ax[1].imshow(depth_img_plot, cmap = 'jet')
      plt.colorbar(dim)
      mim = ax[2].imshow(mask_plot)
      plt.colorbar(mim)

      import time
      ti = time.time()
      plt.savefig("debug_outputs/epoch_val_ftvkitti"+str(ti)+"_"+str(batch_idx)+".jpg")
      plt.close()

    return loss

torch.manual_seed(100)
np.random.seed(100)

# Create dir to save the weights
Path(params['save_dir']).mkdir(parents=True, exist_ok=True)

# Set up tensorboard SummaryWriter and directories
writer = SummaryWriter(os.path.join(params['save_dir'], 'tensorboard'))

"""# Training Loop"""

# Create the DataLoaders for training and validation
train_dataset = DPTData(
    root_dir_list=params['root_dir_list'],
    is_train=True
)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=params['batch_size'], 
    num_workers=6,
    shuffle=True,
    drop_last=True,
    pin_memory=True
)

val_dataset = DPTData(
    root_dir_list=params['val_dir_list'],
    is_train=False,
    )

val_loader = DataLoader(
    dataset=val_dataset, 
    batch_size=params['batch_size'], 
    num_workers=2, 
    shuffle=True, 
    drop_last=True, 
    pin_memory=True)

print('Train set length:', len(train_dataset))
print('Val set lenth:', len(val_dataset))


ft_dpt_model = DPTFinetune()
lr_monitor = LearningRateMonitor(logging_interval='step')
md_ckpt = ModelCheckpoint(every_n_train_steps=500, save_top_k=-1)
trainer = pl.Trainer(check_val_every_n_epoch=None, val_check_interval=500, num_sanity_val_steps=1, max_epochs=params['num_epochs'], callbacks=[lr_monitor, md_ckpt], accelerator='gpu', default_root_dir=params['save_dir'])
trainer.fit(ft_dpt_model, train_loader, val_loader)

exit(0)

#L1_loss = nn.L1Loss().cuda()

start_epoch = 0

def get_s_t(depth_pred, depth_gt):
  depth_pred  = depth_pred.detach().cpu().numpy()
  depth_gt  = depth_gt.detach().cpu().numpy()  
  flattened_pred = depth_pred.flatten()
  flattened_gt = depth_gt.flatten()
  pred_mat = np.vstack([flattened_pred, np.ones(len(flattened_pred))]).T
  s, t = np.linalg.lstsq(pred_mat, flattened_gt, rcond=None)[0]
  return s, t



# TRAINING AND VALIDATION
# scale_sum = 0
# trans_sum = 0
# params['num_epochs'] = 1
for epoch in range(start_epoch, params['num_epochs']):
  print(epoch)
  epoch_loss = 0

  # TRAINING
  model.train()
  #train_loop = tqdm(train_loader, leave=False, position=0)
  #train_loop.set_description(f"Epoch {epoch}/{params['num_epochs']}")
  for batch_idx, batch_data in enumerate(tqdm(train_loader)):
    
    # Load the data
    input_img = batch_data['input_img'].cuda()
    depth_img = batch_data['depth_img'].cuda()

    # Check the current step
    current_step = epoch * len(train_loader) + batch_idx

    #########
    # Train #
    #########

    # Zero the gradients
    optimizer.zero_grad()

    # Forward pass through the entire model (both parts)
    out = model(input_img)
    #out = mout / torch.max(mout)
    out = out * params['scale'] + params['trans']
    #out[out>100] = 100
    out = torch.unsqueeze(out, 1)

    # out_plot = out[0, 0, :, :].detach().cpu()
    # depth_img_plot = depth_img[0, 0, :, :].cpu()
    # fig, ax = plt.subplots(2)
    # oim = ax[0].imshow(out_plot, cmap = 'jet')
    # plt.colorbar(oim)
    # dim = ax[1].imshow(depth_img_plot, cmap = 'jet')
    # plt.colorbar(dim)
    # plt.savefig("debug_outputs/epoch_"+str(batch_idx)+".jpg")
    # plt.close()

    # Calculate losses between gt and output
    loss = 0

    #print(torch.isfinite(out).all(), torch.isfinite())

    
    # Backwards pass and step
    loss.backward()
    optimizer.step()

    # Log
    #out_masked = out[depth_img>0]
    #depth_img_masked = depth_img[depth_img>0]
    epoch_loss += loss#get_sum_loss(out_masked, depth_img_masked, L1_loss).item()
    loss_log = loss.item()

    # Tensorboard
    if (current_step % params['tensorboard_log_step_train']) == 0:
      
      # Log loss
      writer.add_scalar('loss/train', loss_log, current_step)

      if params['masked_ssi_loss_weight']:
        writer.add_scalar('masked_ssi_loss/train', loss_ssi_masked_log, current_step)
      if params['smooth_loss_weight']:
        writer.add_scalar('smooth_loss/train', loss_smooth_log, current_step)
      if params['ssi_trim_loss_weight']:
        writer.add_scalar('ssi_trim_loss/train', loss_ssi_trim, current_step)
      if params['scale_inv_grad_loss_weight']:
        writer.add_scalar('scale_inv_grad_loss/train', loss_scale_inv_grad_log, current_step)
  
  # Print info
  print(
  f"Epoch: {epoch}\n"
  f"Train Loss: {epoch_loss / len(train_loader):.4f}\n"
  f"Learning Rate First {optimizer.param_groups[0]['lr']:.8f}\t"
  )
  out_plot = out[0, 0, :, :].detach().cpu()
  depth_img_plot = depth_img[0, 0, :, :].cpu()
  plt.figure()
  plt.imshow(input_img[0, :, :, :].cpu().permute(1, 2, 0) * .5 + .5)
  plt.figure()
  plt.imshow(out_plot, cmap = 'jet')
  plt.colorbar()
  plt.figure()
  plt.imshow(depth_img_plot, cmap = 'jet')
  plt.colorbar()
  plt.savefig("epoch_"+str(epoch)+".jpg")
  plt.close()

  ##############
  # Validation #
  ##############

  if epoch %  params['val_epoch'] == 0:
    model.eval()
    epoch_loss = 0

    val_loop = tqdm(val_loader, leave=False, position=0)
    val_loop.set_description('Val Epoch')
    for batch_idx, batch_data in enumerate(val_loop):
      
      # Load data
      input_img = batch_data['input_img'].cuda()
      depth_img = batch_data['depth_img'].cuda()

      # Check the current step
      current_step = epoch * len(val_loader) + batch_idx

      # Forward pass of model
      with torch.no_grad():
        out = model(input_img)
        #out = mout / torch.max(mout)
        out = out * params['scale'] + params['trans']
        #out[out>100] = 100
        out = torch.unsqueeze(out, 1)

        # Calculate losses between pseudo-gt and output
        loss = 0

        if params['masked_ssi_loss_weight']:
          out_masked = out[torch.logical_and(depth_img>0, out>0)]
          depth_img_masked = depth_img[torch.logical_and(depth_img>0, out>0)]
          loss_ssi_masked = get_sum_loss(out_masked, depth_img_masked, criterion_masked_ssi)
          loss += params['masked_ssi_loss_weight'] * loss_ssi_masked
          loss_ssi_masked_log = loss_ssi_masked.item()

        if params['smooth_loss_weight']:
          loss_smooth = SmoothLoss(out, input_img)
          loss += params['smooth_loss_weight'] * loss_smooth
          loss_smooth_log = loss_smooth.item()

        if params['ssi_trim_loss_weight']:
          loss_ssi_trim = get_sum_loss(out, depth_img, ssi_trim)
          loss += params['ssi_trim_loss_weight'] * loss_ssi_trim
          loss_ssi_trim_log = loss_ssi_trim.item()

        if params['scale_inv_grad_loss_weight']:
          loss_scale_inv_grad = get_sum_loss(out, depth_img, scale_inv_grad_loss)
          loss += params['scale_inv_grad_loss_weight'] * loss_scale_inv_grad
          loss_scale_inv_grad_log = loss_scale_inv_grad.item()


        # if params['ssim_loss_weight']:
        #   loss_ssim = get_sum_loss(second_stage_out, target_img, criterion_neg_ssim)
        #   loss += params['ssim_loss_weight'] * loss_ssim
        #   loss_ssim_second_log = loss_ssim.item()
        
        #out_masked = out[depth_img>0]
        #depth_img_masked = depth_img[depth_img>0]
        epoch_loss += loss#get_sum_loss(out_masked, depth_img_masked, L1_loss).item()
        loss_log = loss.item()
      
        # Tensorboard
        if (current_step % params['tensorboard_log_step_val']) == 0:
          
          # Log loss
          writer.add_scalar('loss_second/val', loss_log, current_step)

          # Seperate loss
          if params['masked_ssi_loss_weight']:
            writer.add_scalar('masked_ssi_loss/val', loss_ssi_masked_log, current_step)
          if params['smooth_loss_weight']:
            writer.add_scalar('smooth_loss/val', loss_smooth_log, current_step)
          if params['ssi_trim_loss_weight']:
            writer.add_scalar('ssi_trim_loss/train', loss_ssi_trim, current_step)
          if params['scale_inv_grad_loss_weight']:
            writer.add_scalar('scale_inv_grad_loss/train', loss_scale_inv_grad_log, current_step)
    # Print info
    avg_val_loss = epoch_loss / len(val_loader)
    print(
        f"Val Epoch\t"
        f"Val Loss: {avg_val_loss:.4f}"
        )
    
    # Log images 
    out_plot = out[0, 0, :, :].cpu()
    depth_img_plot = depth_img[0, 0, :, :].cpu()
    plt.figure()
    plt.imshow(input_img[0, :, :, :].cpu().permute(1, 2, 0) * .5 + .5)
    plt.figure()
    plt.imshow(out_plot, cmap = 'jet')
    plt.colorbar()
    plt.figure()
    plt.imshow(depth_img_plot, cmap = 'jet')
    plt.colorbar()
    plt.savefig("val_epoch_"+str(epoch)+".jpg")
    plt.close()
  
  # Move the scheduler forward

  # Save every few epochs
  if epoch % params['save_every'] == 0:
    print('Saving...')
    torch.save({
        'epoch': epoch, 
        'state_dict': model.state_dict(),
        'optimizer' : optimizer.state_dict()}, 
        os.path.join(params['save_dir'], f'model_epoch_{epoch}.pth')
        )
  
  # Tensorboard
  writer.flush()
# Close tensorboard
writer.close()

